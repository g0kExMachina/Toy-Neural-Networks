{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A XOR learning ML using Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the training Data\n",
    "\n",
    "x_orig = np.array([[[0],[0]],[[0],[1]], [[1],[0]],[[1],[1]]])\n",
    "y_orig = np.array([[0],[1],[1],[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the input to fit the model\n",
    "\n",
    "x = x_orig.reshape(x_orig.shape[0], -1).T\n",
    "\n",
    "y = y_orig.reshape(y_orig.shape[0], -1).T\n",
    "\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Sigmoid function\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize zeros\n",
    "\n",
    "def zero(dim):\n",
    "    w = np.zeros(shape=(dim,1))\n",
    "    b = 0\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    # calculate A = sigmoid + z\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    # calculate cost\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A))) \n",
    "    \n",
    "    #backpropagation\n",
    "    \n",
    "    dw = 1/m  * np.dot(X,(A-Y).T)\n",
    "    db = 1/m  * np.sum(A-Y)\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return grads,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w,b,X,Y,num_iteration,l_r):\n",
    "    costs =[]\n",
    "    for i in range(num_iteration):\n",
    "        grads,cost = propagate(w,b,X,Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        w = w - l_r * dw\n",
    "        b = b - l_r * db\n",
    "        \n",
    "        if i % 100 ==0 :\n",
    "            costs.append(cost)\n",
    "            print(\"Cost at iteration %i:%f \" %(i,cost))\n",
    "            \n",
    "    params = {\"w\":w,\"b\": b}\n",
    "        \n",
    "    return params,grads,cost\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,X):\n",
    "    m = X.shape[1]\n",
    "    Y_guess = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    #predict\n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_guess[0,i] = 0 if A[0,i] < 0.5 else 1\n",
    "        \n",
    "    return Y_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,num_iteration=200,l_r=0.01):\n",
    "    \n",
    "    w,b = zero(X.shape[0])\n",
    "    params,grads,costs = optimize(w,b,X,Y,num_iteration,l_r)\n",
    "    \n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    Y_predict = predict(w,b,X)\n",
    "    print(Y_predict)\n",
    "    \n",
    "#     print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict - Y)) * 100))\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_train\" : Y_predict, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : l_r,\n",
    "         \"num_iterations\": num_iteration}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0:0.693147 \n",
      "Cost at iteration 100:0.693147 \n",
      "Cost at iteration 200:0.693147 \n",
      "Cost at iteration 300:0.693147 \n",
      "Cost at iteration 400:0.693147 \n",
      "Cost at iteration 500:0.693147 \n",
      "Cost at iteration 600:0.693147 \n",
      "Cost at iteration 700:0.693147 \n",
      "Cost at iteration 800:0.693147 \n",
      "Cost at iteration 900:0.693147 \n",
      "Cost at iteration 1000:0.693147 \n",
      "Cost at iteration 1100:0.693147 \n",
      "Cost at iteration 1200:0.693147 \n",
      "Cost at iteration 1300:0.693147 \n",
      "Cost at iteration 1400:0.693147 \n",
      "Cost at iteration 1500:0.693147 \n",
      "Cost at iteration 1600:0.693147 \n",
      "Cost at iteration 1700:0.693147 \n",
      "Cost at iteration 1800:0.693147 \n",
      "Cost at iteration 1900:0.693147 \n",
      "Cost at iteration 2000:0.693147 \n",
      "Cost at iteration 2100:0.693147 \n",
      "Cost at iteration 2200:0.693147 \n",
      "Cost at iteration 2300:0.693147 \n",
      "Cost at iteration 2400:0.693147 \n",
      "Cost at iteration 2500:0.693147 \n",
      "Cost at iteration 2600:0.693147 \n",
      "Cost at iteration 2700:0.693147 \n",
      "Cost at iteration 2800:0.693147 \n",
      "Cost at iteration 2900:0.693147 \n",
      "Cost at iteration 3000:0.693147 \n",
      "Cost at iteration 3100:0.693147 \n",
      "Cost at iteration 3200:0.693147 \n",
      "Cost at iteration 3300:0.693147 \n",
      "Cost at iteration 3400:0.693147 \n",
      "Cost at iteration 3500:0.693147 \n",
      "Cost at iteration 3600:0.693147 \n",
      "Cost at iteration 3700:0.693147 \n",
      "Cost at iteration 3800:0.693147 \n",
      "Cost at iteration 3900:0.693147 \n",
      "Cost at iteration 4000:0.693147 \n",
      "Cost at iteration 4100:0.693147 \n",
      "Cost at iteration 4200:0.693147 \n",
      "Cost at iteration 4300:0.693147 \n",
      "Cost at iteration 4400:0.693147 \n",
      "Cost at iteration 4500:0.693147 \n",
      "Cost at iteration 4600:0.693147 \n",
      "Cost at iteration 4700:0.693147 \n",
      "Cost at iteration 4800:0.693147 \n",
      "Cost at iteration 4900:0.693147 \n",
      "Cost at iteration 5000:0.693147 \n",
      "Cost at iteration 5100:0.693147 \n",
      "Cost at iteration 5200:0.693147 \n",
      "Cost at iteration 5300:0.693147 \n",
      "Cost at iteration 5400:0.693147 \n",
      "Cost at iteration 5500:0.693147 \n",
      "Cost at iteration 5600:0.693147 \n",
      "Cost at iteration 5700:0.693147 \n",
      "Cost at iteration 5800:0.693147 \n",
      "Cost at iteration 5900:0.693147 \n",
      "Cost at iteration 6000:0.693147 \n",
      "Cost at iteration 6100:0.693147 \n",
      "Cost at iteration 6200:0.693147 \n",
      "Cost at iteration 6300:0.693147 \n",
      "Cost at iteration 6400:0.693147 \n",
      "Cost at iteration 6500:0.693147 \n",
      "Cost at iteration 6600:0.693147 \n",
      "Cost at iteration 6700:0.693147 \n",
      "Cost at iteration 6800:0.693147 \n",
      "Cost at iteration 6900:0.693147 \n",
      "Cost at iteration 7000:0.693147 \n",
      "Cost at iteration 7100:0.693147 \n",
      "Cost at iteration 7200:0.693147 \n",
      "Cost at iteration 7300:0.693147 \n",
      "Cost at iteration 7400:0.693147 \n",
      "Cost at iteration 7500:0.693147 \n",
      "Cost at iteration 7600:0.693147 \n",
      "Cost at iteration 7700:0.693147 \n",
      "Cost at iteration 7800:0.693147 \n",
      "Cost at iteration 7900:0.693147 \n",
      "Cost at iteration 8000:0.693147 \n",
      "Cost at iteration 8100:0.693147 \n",
      "Cost at iteration 8200:0.693147 \n",
      "Cost at iteration 8300:0.693147 \n",
      "Cost at iteration 8400:0.693147 \n",
      "Cost at iteration 8500:0.693147 \n",
      "Cost at iteration 8600:0.693147 \n",
      "Cost at iteration 8700:0.693147 \n",
      "Cost at iteration 8800:0.693147 \n",
      "Cost at iteration 8900:0.693147 \n",
      "Cost at iteration 9000:0.693147 \n",
      "Cost at iteration 9100:0.693147 \n",
      "Cost at iteration 9200:0.693147 \n",
      "Cost at iteration 9300:0.693147 \n",
      "Cost at iteration 9400:0.693147 \n",
      "Cost at iteration 9500:0.693147 \n",
      "Cost at iteration 9600:0.693147 \n",
      "Cost at iteration 9700:0.693147 \n",
      "Cost at iteration 9800:0.693147 \n",
      "Cost at iteration 9900:0.693147 \n",
      "[[1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "d = model(x,y,num_iteration=10000,l_r=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lession Learned: Pratically Useless.\n",
    "This predicts 1 for all inputs and gets half of them correct.\n",
    "### Why?\n",
    "-------------------------------\n",
    "* The xor pattern is not linearly separable. So logistic regression can only work at best 75 % . To get 100 % accurate prediction. Add a feature x1 * x2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
